{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6086165d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Projects under 'ucph-simonwin':\n",
      "    • uncategorized\n",
      "    • MedicalSegmentation\n",
      "    • Sweap1\n",
      "    • VariableDepthNet-Sweep\n"
     ]
    }
   ],
   "source": [
    "import wandb\n",
    "\n",
    "api = wandb.Api()\n",
    "\n",
    "# Try listing projects under \"simonwa01\"\n",
    "entity_to_check = \"ucph-simonwin\"\n",
    "projects = api.projects(entity_to_check)\n",
    "\n",
    "print(f\"Projects under '{entity_to_check}':\")\n",
    "for proj in projects:\n",
    "    print(\"    •\", proj.name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa11ac3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "# ─────────────────────────────────────────────────────────────────────\n",
    "# 1) Entity & project exactly as in W&B\n",
    "# ─────────────────────────────────────────────────────────────────────\n",
    "ENTITY_NAME  = \"ucph-simonwin\"\n",
    "PROJECT_NAME = \"MedicalSegmentation\"\n",
    "PROJECT_PATH = f\"{ENTITY_NAME}/{PROJECT_NAME}\"\n",
    "\n",
    "# ─────────────────────────────────────────────────────────────────────\n",
    "# 2) Initialize W&B API (assumes wandb.login() has been done already)\n",
    "# ─────────────────────────────────────────────────────────────────────\n",
    "api = wandb.Api()\n",
    "\n",
    "# ─────────────────────────────────────────────────────────────────────\n",
    "# 3) Fetch all runs in the project\n",
    "# ─────────────────────────────────────────────────────────────────────\n",
    "all_runs = api.runs(PROJECT_PATH)\n",
    "\n",
    "# ─────────────────────────────────────────────────────────────────────\n",
    "# 4) Build the Plotly figure (same filtering: peak ≥ 0.8, y-axis 0.8–1.0)\n",
    "# ─────────────────────────────────────────────────────────────────────\n",
    "fig = go.Figure()\n",
    "\n",
    "for run in all_runs:\n",
    "    try:\n",
    "        df = run.history(keys=[\"epoch\", \"val/dice_fullres\"], pandas=True)\n",
    "    except Exception:\n",
    "        continue\n",
    "    if df is None or df.shape[0] == 0:\n",
    "        continue\n",
    "\n",
    "    df = df.dropna(subset=[\"epoch\", \"val/dice_fullres\"])\n",
    "    if df.shape[0] == 0:\n",
    "        continue\n",
    "\n",
    "    if df[\"val/dice_fullres\"].max() < 0.8:\n",
    "        continue\n",
    "\n",
    "    hover_text = [f\"{run.name}<br>Dice: {v:.3f}\" for v in df[\"val/dice_fullres\"]]\n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=df[\"epoch\"],\n",
    "        y=df[\"val/dice_fullres\"],\n",
    "        mode='lines',\n",
    "        name=run.name,\n",
    "        hoverinfo='text',\n",
    "        hovertext=hover_text,\n",
    "        line=dict(width=1),\n",
    "        opacity=0.6\n",
    "    ))\n",
    "\n",
    "fig.update_layout(\n",
    "    title=f\"Interactive Plot: Runs in {ENTITY_NAME}/{PROJECT_NAME} (peak ≥ 0.8)\",\n",
    "    xaxis_title=\"Epoch\",\n",
    "    yaxis_title=\"val/dice_fullres\",\n",
    "    yaxis=dict(range=[0.8, 1.0]),\n",
    "    legend_title=\"Run Name\",\n",
    "    hovermode=\"closest\",\n",
    "    width=900,\n",
    "    height=600\n",
    ")\n",
    "\n",
    "# ─────────────────────────────────────────────────────────────────────\n",
    "# 5) Export to HTML instead of fig.show()\n",
    "# ─────────────────────────────────────────────────────────────────────\n",
    "html_path = \"wandb_dice_plot.html\"\n",
    "fig.write_html(html_path, include_plotlyjs=\"cdn\")\n",
    "print(f\"Saved interactive plot to '{html_path}'.\\nOpen that file in your browser to view it.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e00e24d",
   "metadata": {},
   "source": [
    "# <span style=\"color:#fcba03\">Leaderboards</span>:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b41a9fa0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datasets found in this project:\n",
      "  • Task01_BrainTumour\n",
      "  • Task04_Hippocampus\n",
      "\n",
      "=== Leaderboard for metric: val/dice_fullres  |  Dataset: Task01_BrainTumour ===\n",
      "\n",
      "Architecture: ms-unet3d\n",
      "  1. lr_d6_w40_dropout0_a0.2_es-dice_p30      0.7264   →  trained_models/ms-unet3d/Task01_BrainTumour/2025-05-31_14:08:48_lr_d6_w40_dropout0_a0.2_es-dice_p30/\n",
      "  2. lr_RePl_d6_w32_dropout0                  0.7208   →  trained_models/ms-unet3d/Task01_BrainTumour/2025-05-31_07:56:44_lr_RePl_d6_w32_dropout0/\n",
      "  3. lr-RP_d6_w45_dropout0_a0.3_es-dice_p35   0.7187   →  trained_models/ms-unet3d/Task01_BrainTumour/2025-06-01_16:02:29_lr-RP_d6_w45_dropout0_a0.3_es-dice_p35/\n",
      "\n",
      "Architecture: unet-aug3d\n",
      "  1. bb-aug_d6_w30_dropout0         0.4889   →  trained_models/unet-aug3d/Task01_BrainTumour/2025-05-30_13:08:23_bb-aug_d6_w30_dropout0/\n",
      "  2. bb-aug_d6_w40_dropout0_a0.25   0.3413   →  <no path found>/\n",
      "  3. bb-aug_d6_w32_dropout0         0.3107   →  trained_models/unet-aug3d/Task01_BrainTumour/2025-05-30_12:21:41_bb-aug_d6_w32_dropout0/\n",
      "\n",
      "Architecture: unet3d\n",
      "  1. bb_lr_RePl_d6_w32_dropout0   0.6787   →  trained_models/unet3d/Task01_BrainTumour/2025-06-02_10:43:59_bb_lr_RePl_d6_w32_dropout0/\n",
      "  2. bb_d6_w33_dropout0           0.6562   →  trained_models/unet3d/Task01_BrainTumour/2025-05-30_10:21:06_bb_d6_w33_dropout0/\n",
      "  3. bb_d6_w40_dropout0_a0.25     0.5361   →  <no path found>/\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import time\n",
    "import wandb\n",
    "from requests.exceptions import HTTPError\n",
    "\n",
    "ENTITY_NAME  = \"ucph-simonwin\"\n",
    "PROJECT_NAME = \"MedicalSegmentation\"\n",
    "PROJECT_PATH = f\"{ENTITY_NAME}/{PROJECT_NAME}\"\n",
    "api = wandb.Api()\n",
    "all_runs = api.runs(PROJECT_PATH)\n",
    "\n",
    "def safe_history(run, keys, attempts=3):\n",
    "    for i in range(attempts):\n",
    "        try:\n",
    "            return run.history(keys=keys, pandas=True)\n",
    "        except HTTPError:\n",
    "            time.sleep(2 ** i)\n",
    "    return None\n",
    "\n",
    "def safe_files(run, attempts=3):\n",
    "    for i in range(attempts):\n",
    "        try:\n",
    "            return run.files()\n",
    "        except HTTPError:\n",
    "            time.sleep(2 ** i)\n",
    "    return []\n",
    "\n",
    "# Step 1: Build dataset‐filtered leaderboard\n",
    "target_names = [\"ms-unet3d\", \"unet-aug3d\", \"unet3d\"]\n",
    "metrics = {\"val/dice_fullres\": {\"higher_is_better\": True}}\n",
    "\n",
    "seen_datasets = set()\n",
    "for run in all_runs:\n",
    "    cfg = getattr(run, \"config\", {}) or {}\n",
    "    raw_dataset = cfg.get(\"dataset\") if isinstance(cfg.get(\"dataset\"), str) else run.group or \"\"\n",
    "    if raw_dataset:\n",
    "        seen_datasets.add(raw_dataset)\n",
    "    time.sleep(0.1)\n",
    "\n",
    "if not seen_datasets:\n",
    "    print(\"No runs have config['dataset'] or run.group. Exiting.\")\n",
    "    exit()\n",
    "\n",
    "print(\"Datasets found in this project:\")\n",
    "for d in sorted(seen_datasets):\n",
    "    print(\"  •\", d)\n",
    "\n",
    "chosen_dataset = input(\"\\nType exactly the dataset name from above and press Enter: \").strip()\n",
    "if chosen_dataset not in seen_datasets:\n",
    "    print(f\"❌ '{chosen_dataset}' not in the list. Exiting.\")\n",
    "    exit()\n",
    "\n",
    "results = {m: {arch: [] for arch in target_names} for m in metrics}\n",
    "\n",
    "for run in all_runs:\n",
    "    cfg = getattr(run, \"config\", {}) or {}\n",
    "    arch_cfg = cfg.get(\"architecture\", {})\n",
    "    if not isinstance(arch_cfg, dict):\n",
    "        time.sleep(0.1)\n",
    "        continue\n",
    "\n",
    "    model_name = arch_cfg.get(\"name\")\n",
    "    if model_name not in target_names:\n",
    "        time.sleep(0.1)\n",
    "        continue\n",
    "\n",
    "    raw_dataset = cfg.get(\"dataset\") if isinstance(cfg.get(\"dataset\"), str) else run.group or \"\"\n",
    "    if raw_dataset != chosen_dataset:\n",
    "        time.sleep(0.1)\n",
    "        continue\n",
    "\n",
    "    df = safe_history(run, list(metrics.keys()))\n",
    "    if df is None or df.empty:\n",
    "        time.sleep(0.1)\n",
    "        continue\n",
    "\n",
    "    for metric_name, props in metrics.items():\n",
    "        if metric_name not in df.columns:\n",
    "            continue\n",
    "        series = df[metric_name].dropna()\n",
    "        if series.empty:\n",
    "            continue\n",
    "        best_val = series.max() if props[\"higher_is_better\"] else series.min()\n",
    "        results[metric_name][model_name].append((run.name, best_val))\n",
    "\n",
    "    time.sleep(0.5)\n",
    "\n",
    "# Step 2: Scan logs to find each run’s “trained_models/.../best_model.pth” path\n",
    "pattern = re.compile(r\"(trained_models/.+?)/best_model\\.pth\")\n",
    "download_dir = \"wandb_logs\"\n",
    "os.makedirs(download_dir, exist_ok=True)\n",
    "\n",
    "run_to_paths = {}\n",
    "\n",
    "for run in all_runs:\n",
    "    cfg = getattr(run, \"config\", {}) or {}\n",
    "    raw_dataset = cfg.get(\"dataset\") if isinstance(cfg.get(\"dataset\"), str) else run.group or \"\"\n",
    "    if raw_dataset != chosen_dataset:\n",
    "        time.sleep(0.1)\n",
    "        continue\n",
    "\n",
    "    files = safe_files(run)\n",
    "    log_files = [f for f in files if f.name.lower() == \"output.log\"]\n",
    "    if not log_files:\n",
    "        time.sleep(0.1)\n",
    "        continue\n",
    "\n",
    "    log_file = log_files[0]\n",
    "    local_filename = f\"{run.id}_output.log\"\n",
    "    local_path = os.path.join(download_dir, local_filename)\n",
    "\n",
    "    # Always download fresh, overwrite if exists\n",
    "    try:\n",
    "        log_file.download(root=download_dir, replace=True)\n",
    "    except TypeError:\n",
    "        log_file.download(root=download_dir, exist_ok=True)\n",
    "\n",
    "    # After download, W&B writes \"wandb_logs/output.log\". Rename to run-specific file\n",
    "    downloaded = os.path.join(download_dir, \"output.log\")\n",
    "    if os.path.exists(downloaded):\n",
    "        try:\n",
    "            os.replace(downloaded, local_path)\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "    file_to_open = local_path if os.path.exists(local_path) else None\n",
    "    if not file_to_open:\n",
    "        time.sleep(0.1)\n",
    "        continue\n",
    "\n",
    "    try:\n",
    "        with open(file_to_open, \"r\", encoding=\"utf-8\", errors=\"ignore\") as fp:\n",
    "            content = fp.read()\n",
    "    except Exception:\n",
    "        time.sleep(0.1)\n",
    "        continue\n",
    "\n",
    "    matches = pattern.findall(content)\n",
    "    if matches:\n",
    "        run_to_paths[run.name] = sorted(set(matches))\n",
    "\n",
    "    time.sleep(0.5)\n",
    "\n",
    "# Step 3: Print aligned leaderboard with paths\n",
    "# ─ Step 3: Print “aligned” leaderboard with paths ────────────────────────\n",
    "for metric_name, model_dict in results.items():\n",
    "    print(f\"\\n=== Leaderboard for metric: {metric_name}  |  Dataset: {chosen_dataset} ===\\n\")\n",
    "    for model_name in target_names:\n",
    "        run_list = model_dict.get(model_name, [])\n",
    "        if not run_list:\n",
    "            print(f\"Architecture '{model_name}': NO runs found in '{chosen_dataset}'.\\n\")\n",
    "            continue\n",
    "\n",
    "        # Sort and take top 3\n",
    "        reverse_sort = metrics[metric_name][\"higher_is_better\"]\n",
    "        sorted_runs = sorted(run_list, key=lambda x: x[1], reverse=reverse_sort)[:3]\n",
    "\n",
    "        # Find the longest run name among these three, so we can pad accordingly\n",
    "        max_name_len = max(len(rn) for rn, _ in sorted_runs)\n",
    "\n",
    "        print(f\"Architecture: {model_name}\")\n",
    "        for rank, (run_name, val) in enumerate(sorted_runs, start=1):\n",
    "            paths = run_to_paths.get(run_name, [\"<no path found>\"])\n",
    "            path_display = \"; \".join(p + \"/\" for p in paths)\n",
    "            # Pad run_name to max_name_len, right‐align \"val\" in a 7-char field with 4 decimals\n",
    "            print(\n",
    "                f\"  {rank}. \"\n",
    "                f\"{run_name:<{max_name_len}}  \"\n",
    "                f\"{val:>7.4f}   →  {path_display}\"\n",
    "            )\n",
    "        print()\n",
    "        \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
